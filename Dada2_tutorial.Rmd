---
title: "Dada2 tutorial"
output:
  pdf_document: default
  html_notebook: default
---

#Chargement du package Dada2

```{r}
library(dada2); packageVersion("dada2")
```
On voit la version du package de dada2 (1.16.0)


#Chargement des fichiers fastq à partir du séquençage d'amplicons de la région V4 de l'ARN 16S de souris générés par la technique de Illumina MiSeq. On a défini le chemin. 
```{r}
path <- "~/MiSeq_SOP" # CHANGE ME to the directory containing the fastq files after unzipping.
list.files(path)
```

#Obtention des "forward" et "reverse"
```{r}
# Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq
fnFs <- sort(list.files(path, pattern="_R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001.fastq", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```


#Maintenant, on peut obtenir la qualité de lecture
```{r}
plotQualityProfile(fnFs[1:2])
```
Nous obtenons les profils des scores de qualité en fonction du nombre de cycle des fichiers fastq des forward reads.

```{r}
plotQualityProfile(fnRs[1:2])
```
Nous obtenons les profils des scores de qualité en fonction du nombre de cycle des fichiers fastq des reverse reads. On remarque que ces reverse reads sont de moins bonne qualité que les forward. 


#Filtering and trimming
```{r}
# Place filtered files in filtered/ subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```


```{r}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(240,160),
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE
head(out)
```
Cela permet de filtrer les données selon un taux maximal d'acceptation d'erreurs. On a donné des reads, et ceux qui sont sortis sont ceux que l'on garde.


#Errors Rates
```{r}
errF <- learnErrors(filtFs, multithread=TRUE)
```

```{r}
errR <- learnErrors(filtRs, multithread=TRUE)
```

#Visualisation du taux d'erreurs estimés
```{r}
plotErrors(errF, nominalQ=TRUE)
```

#Sample Inference 20 samples (Forward and Reverse)
```{r}
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
```

```{r}
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)
```

```{r}
dadaFs[[1]]
```

```{r}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
```

```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```

```{r}
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
```

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
```

```{r}
sum(seqtab.nochim)/sum(seqtab)
```

```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```

```{r}
taxa <- assignTaxonomy(seqtab.nochim, "~/tax/silva_nr_v132_train_set.fa.gz", multithread=TRUE)
```
```{r}
silva_nr_v132_train_set.fa.gz
```
```{r}
wget silva_nr_v132_train_set.fa.gz
```

```{bash}
wget silva_nr_v132_train_set.fa.gz
```

```{bash}
wget https://zenodo.org/record/3986799/files/silva_nr99_v138_train_set.fa.gz
```

```{r}
taxa <- assignTaxonomy(seqtab.nochim, "~/tax/silva_nr99_v138_train_set.fa.gz", multithread=TRUE)
```

```{r}
taxa <- assignTaxonomy(seqtab.nochim, "~/silva_nr99_v138_train_set.fa.gz", multithread=TRUE)
```

```{r}
taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
```

```{r}
unqs.mock <- seqtab.nochim["Mock",]
unqs.mock <- sort(unqs.mock[unqs.mock>0], decreasing=TRUE) # Drop ASVs absent in the Mock
cat("DADA2 inferred", length(unqs.mock), "sample sequences present in the Mock community.\n")
```

```{r}
mock.ref <- getSequences(file.path(path, "HMP_MOCK.v35.fasta"))
match.ref <- sum(sapply(names(unqs.mock), function(x) any(grepl(x, mock.ref))))
cat("Of those,", sum(match.ref), "were exact matches to the expected reference sequences.\n")
```

```{r}
if(!requireNamespace("BiocManager")){
  install.packages("BiocManager")
}
BiocManager::install("phyloseq")
```

```{bash}
sudo apt-get install -y libglpk-dev
```

```{r}
BiocManager::install("phyloseq")
```

```{r}
library(phyloseq); packageVersion("phyloseq")
```

```{r}
library(Biostrings); packageVersion("Biostrings")
```

```{r}
library(ggplot2); packageVersion("ggplot2")
```

```{r}
theme_set(theme_bw())
```

```{r}
samples.out <- rownames(seqtab.nochim)
subject <- sapply(strsplit(samples.out, "D"), `[`, 1)
gender <- substr(subject,1,1)
subject <- substr(subject,2,999)
day <- as.integer(sapply(strsplit(samples.out, "D"), `[`, 2))
samdf <- data.frame(Subject=subject, Gender=gender, Day=day)
samdf$When <- "Early"
samdf$When[samdf$Day>100] <- "Late"
rownames(samdf) <- samples.out
```

```{r}
ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               sample_data(samdf), 
               tax_table(taxa))
ps <- prune_samples(sample_names(ps) != "Mock", ps) # Remove mock sample
```

```{r}
dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
ps
```

```{r}
plot_richness(ps, x="Day", measures=c("Shannon", "Simpson"), color="When")
```

```{r}
# Transform data to proportions as appropriate for Bray-Curtis distances
ps.prop <- transform_sample_counts(ps, function(otu) otu/sum(otu))
ord.nmds.bray <- ordinate(ps.prop, method="NMDS", distance="bray")
```

```{r}
plot_ordination(ps.prop, ord.nmds.bray, color="When", title="Bray NMDS")
```

```{r}
top20 <- names(sort(taxa_sums(ps), decreasing=TRUE))[1:20]
ps.top20 <- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
ps.top20 <- prune_taxa(top20, ps.top20)
```


```{r}
plot_bar(ps.top20, x="Day", fill="Family") + facet_wrap(~When, scales="free_x")
```

```{r}
h1 = 0.4
ps4 = tip_glom(ps2, h = h1)
```

```{r}
ps_connect<-url("https://raw.githubusercontent.com/spholmes/F1000_workflow/master/data/ps.rds")
ps=readRDS(ps_connect)
ps
```

```{r}
library("phyloseqGraphTest")
library("igraph")
library("ggnetwork")
net <- make_network(ps, max.dist=0.35)
sampledata <- data.frame(sample_data(ps))
V(net)$id <- sampledata[names(V(net)), "host_subject_id"]
V(net)$litter <- sampledata[names(V(net)), "family_relationship"]
```

#Package igraph
```{r}
library(igraph)
```

```{r}
net_graph <-ggnetwork(net)

```
```{r}
ggplot(net_graph, aes(x = x, y = y, xend = xend, yend = yend), layout = "fruchtermanreingold") +
  geom_edges(color = "darkgray") +
  geom_nodes(aes(color = id, shape = litter),  size = 3 ) +
  theme(axis.text = element_blank(), axis.title = element_blank(),
        legend.key.height = unit(0.5,"line")) +
  guides(col = guide_legend(override.aes = list(size = .5)))
```

```{bash}
wget https://cran.r-project.org/src/contrib/Archive/structSSI/structSSI_1.1.1.tar.gz
```
```{r}
library("reshape2")
library("DESeq2")
#New version of DESeq2 needs special levels
sample_data(ps)$age_binned <- cut(sample_data(ps)$age,
                          breaks = c(0, 100, 200, 400))
levels(sample_data(ps)$age_binned) <- list(Young100="(0,100]", Mid100to200="(100,200]", Old200="(200,400]")
sample_data(ps)$family_relationship = gsub(" ", "", sample_data(ps)$family_relationship)
ps_dds <- phyloseq_to_deseq2(ps, design = ~ age_binned + family_relationship)

# geometric mean, set to zero when all coordinates are zero
geo_mean_protected <- function(x) {
  if (all(x == 0)) {
    return (0)
  }
  exp(mean(log(x[x != 0])))
}

geoMeans <- apply(counts(ps_dds), 1, geo_mean_protected)
ps_dds <- estimateSizeFactors(ps_dds, geoMeans = geoMeans)
ps_dds <- estimateDispersions(ps_dds)
abund <- getVarianceStabilizedData(ps_dds)
```
```{r}
library("structSSI")
el <- phy_tree(pslog)$edge
el0 <- el
el0 <- el0[nrow(el):1, ]
el_names <- c(short_names, seq_len(phy_tree(pslog)$Nnode))
el[, 1] <- el_names[el0[, 1]]
el[, 2] <- el_names[as.numeric(el0[, 2])]
unadj_p <- treePValues(el, abund, sample_data(pslog)$age_binned)

hfdr_res <- hFDR.adjust(unadj_p, el, .75)
summary(hfdr_res)
```

```{r}
which git
## /usr/local/bin/git
```

```{bash}
wget https://github.com/git-for-windows/git/releases/download/v2.29.2.windows.2/Git-2.29.2.2-64-bit.exe
```

```{bash}
which git
```

```{bash}
git --version
```

```{bash}
git config --global user.name 'mfave29'
git config --global user.email 'mfave29@gmail.com'
git config --global --list
```

```{r}
update.packages(ask = FALSE, checkBuilt = TRUE)
```

```{bash}
git config --global user.name 'mfave29'
git config --global user.email 'mfave29@gmail.com'
git config --global --list
```

```{r}
## install if needed (do this exactly once):
## install.packages("usethis")

library(usethis)
use_git_config(user.name = "mfave29", user.email = "mfave29@gmail.com")
```

```{bash}
which git
```

```{bash}
git --version
```

```{bash}
git config --global --list
```

```{bash}
git config --global core.editor "emacs"
```

```{bash}
git clone https://github.com/mfave29/myrepo.git
```

```{r}
git clone https://github.com/mfave29/myrepo.git
```
```{bash}
cd myrepo
ls
head README.md
git remote show origin
```

```{bash}
echo "A line I wrote on my local computer" >> README.md
git status
```
```{bash}
echo "A line I wrote on my local computer" >> README.md
```


```{bash}
git status
```
```{bash}
ls -a
```

```{bash}
cd myrepo
ls
head README.md
git remote show origin
```

```{bash}
echo "A line I wrote on my local computer" >> README.md
git status
```

```{bash}
cd myrepo
ls
head README.md
git remote show origin
```

```{bash}
echo "A line I wrote on my local computer" >> README.md
git status
```

```{bash}
git add -A
git commit -m "A commit from my local computer"
git push
```




